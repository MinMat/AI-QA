{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (4.30.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.9.69)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.30.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (0.4.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.14.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.20.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.69 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.12.69)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.1.13)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2018.8.24)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.69->boto3->pytorch-pretrained-bert) (2.7.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.69->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.69->boto3->pytorch-pretrained-bert) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (1.18.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (1.14.5)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (1.12.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (1.0.6)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "!pip install pytorch-pretrained-bert\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMultipleChoice\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question Stem</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Complete Question</th>\n",
       "      <th>Answer Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7-980</td>\n",
       "      <td>The sun is responsible for</td>\n",
       "      <td>(A) puppies learning new tricks (B) children g...</td>\n",
       "      <td>The sun is responsible for (A) puppies learnin...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7-584</td>\n",
       "      <td>When standing miles away from Mount Rushmore</td>\n",
       "      <td>(A) the mountains seem very close (B) the moun...</td>\n",
       "      <td>When standing miles away from Mount Rushmore (...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7-870</td>\n",
       "      <td>When food is reduced in the stomach</td>\n",
       "      <td>(A) the mind needs time to digest (B) take a s...</td>\n",
       "      <td>When food is reduced in the stomach (A) the mi...</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                 Question Stem  \\\n",
       "0  7-980                    The sun is responsible for   \n",
       "1  7-584  When standing miles away from Mount Rushmore   \n",
       "2  7-870           When food is reduced in the stomach   \n",
       "\n",
       "                                             Choices  \\\n",
       "0  (A) puppies learning new tricks (B) children g...   \n",
       "1  (A) the mountains seem very close (B) the moun...   \n",
       "2  (A) the mind needs time to digest (B) take a s...   \n",
       "\n",
       "                                   Complete Question Answer Key  \n",
       "0  The sun is responsible for (A) puppies learnin...          D  \n",
       "1  When standing miles away from Mount Rushmore (...          D  \n",
       "2  When food is reduced in the stomach (A) the mi...          C  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"InferSent/encoder/data/train.tsv\",sep='\\t')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:5: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:6: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:7: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:8: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:21: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:19: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:15: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:17: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
     ]
    }
   ],
   "source": [
    "#Create new features from the answer choices  \n",
    "for index, row in train.iterrows():\n",
    "    ans = re.split(\"(\\([A-Z]\\))\", row['Choices'])\n",
    "    ans.remove('')\n",
    "    train.set_value(index,'OptionA',ans[1])\n",
    "    train.set_value(index,'OptionB',ans[3])\n",
    "    train.set_value(index,'OptionC',ans[5])\n",
    "    train.set_value(index,'OptionD',ans[7])\n",
    "    \n",
    "#Find the answer key and replace it with the value   \n",
    "for index, row in train.iterrows():\n",
    "    ans = re.split(\"(\\([A-Z]\\))\", row['Choices'])\n",
    "    ans.remove('')\n",
    "    if row['Answer Key'] == 'A':\n",
    "        train.set_value(index,'Result',ans[1])\n",
    "    elif row['Answer Key'] == 'B':\n",
    "        train.set_value(index,'Result',ans[3])\n",
    "    elif row['Answer Key'] == 'C':\n",
    "        train.set_value(index,'Result',ans[5])\n",
    "    elif row['Answer Key'] == 'D':\n",
    "        train.set_value(index,'Result',ans[7]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_categories = 4957\n",
    "shuffled = train.reindex(np.random.permutation(train.index))\n",
    "a = shuffled[shuffled['Answer Key'] == 'A'][:num_of_categories]\n",
    "b = shuffled[shuffled['Answer Key'] == 'B'][:num_of_categories]\n",
    "c = shuffled[shuffled['Answer Key'] == 'C'][:num_of_categories]\n",
    "d = shuffled[shuffled['Answer Key'] == 'D'][:num_of_categories]\n",
    "concated = pd.concat([a,b,c,d], ignore_index=True)\n",
    "#Shuffle the dataset\n",
    "concated = concated.reindex(np.random.permutation(concated.index))\n",
    "concated['LABEL'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355    2\n",
      "2405    1\n",
      "4373    3\n",
      "107     0\n",
      "3802    3\n",
      "39      0\n",
      "2522    1\n",
      "3873    3\n",
      "1762    1\n",
      "2224    1\n",
      "Name: LABEL, dtype: int64\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "#One-hot encode the labels\n",
    "concated.loc[concated['Answer Key'] == 'A', 'LABEL'] = 0\n",
    "concated.loc[concated['Answer Key'] == 'B', 'LABEL'] = 1\n",
    "concated.loc[concated['Answer Key'] == 'C','LABEL'] = 2\n",
    "concated.loc[concated['Answer Key'] == 'D','LABEL'] = 3\n",
    "print(concated['LABEL'][:10])\n",
    "labels = to_categorical(concated['LABEL'], num_classes=4)\n",
    "print(labels[:10])\n",
    "if 'CATEGORY' in concated.keys():\n",
    "    concated.drop(['CATEGORY'], axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Question Stem</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Complete Question</th>\n",
       "      <th>Answer Key</th>\n",
       "      <th>OptionA</th>\n",
       "      <th>OptionB</th>\n",
       "      <th>OptionC</th>\n",
       "      <th>OptionD</th>\n",
       "      <th>Result</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3355</th>\n",
       "      <td>12-1201</td>\n",
       "      <td>Which are likely slightly larger in size than ...</td>\n",
       "      <td>(A) solid steel (B) solid wood (C) solid glaci...</td>\n",
       "      <td>Which are likely slightly larger in size than ...</td>\n",
       "      <td>C</td>\n",
       "      <td>solid steel</td>\n",
       "      <td>solid wood</td>\n",
       "      <td>solid glaciers</td>\n",
       "      <td>solid mercury</td>\n",
       "      <td>solid glaciers</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2405</th>\n",
       "      <td>13-508</td>\n",
       "      <td>What does a stove generate for cooking?</td>\n",
       "      <td>(A) ice (B) scorching temperatures (C) cold (D...</td>\n",
       "      <td>What does a stove generate for cooking? (A) ic...</td>\n",
       "      <td>B</td>\n",
       "      <td>ice</td>\n",
       "      <td>scorching temperatures</td>\n",
       "      <td>cold</td>\n",
       "      <td>freezing</td>\n",
       "      <td>scorching temperatures</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>9-1064</td>\n",
       "      <td>a plant needs this to survive</td>\n",
       "      <td>(A) bugs to eat (B) liquid food (C) finger foo...</td>\n",
       "      <td>a plant needs this to survive (A) bugs to eat ...</td>\n",
       "      <td>D</td>\n",
       "      <td>bugs to eat</td>\n",
       "      <td>liquid food</td>\n",
       "      <td>finger food</td>\n",
       "      <td>nutritional material</td>\n",
       "      <td>nutritional material</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>7-590</td>\n",
       "      <td>An example of weathering is</td>\n",
       "      <td>(A) sand (B) sharp boulders (C) tall trees (D)...</td>\n",
       "      <td>An example of weathering is (A) sand (B) sharp...</td>\n",
       "      <td>A</td>\n",
       "      <td>sand</td>\n",
       "      <td>sharp boulders</td>\n",
       "      <td>tall trees</td>\n",
       "      <td>coral reefs</td>\n",
       "      <td>sand</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>12-918</td>\n",
       "      <td>Echolocation is when animals detect objects by...</td>\n",
       "      <td>(A) smell (B) vision (C) sight (D) screeching</td>\n",
       "      <td>Echolocation is when animals detect objects by...</td>\n",
       "      <td>D</td>\n",
       "      <td>smell</td>\n",
       "      <td>vision</td>\n",
       "      <td>sight</td>\n",
       "      <td>screeching</td>\n",
       "      <td>screeching</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10-397</td>\n",
       "      <td>A large turbine in a field can produce electri...</td>\n",
       "      <td>(A) a breeze is floating (B) a child is crying...</td>\n",
       "      <td>A large turbine in a field can produce electri...</td>\n",
       "      <td>A</td>\n",
       "      <td>a breeze is floating</td>\n",
       "      <td>a child is crying</td>\n",
       "      <td>a dog is barking</td>\n",
       "      <td>clouds are very thin</td>\n",
       "      <td>a breeze is floating</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>8-339</td>\n",
       "      <td>Peat is an important factor when</td>\n",
       "      <td>(A) Driving a new car (B) Putting a fresh rasp...</td>\n",
       "      <td>Peat is an important factor when (A) Driving a...</td>\n",
       "      <td>B</td>\n",
       "      <td>Driving a new car</td>\n",
       "      <td>Putting a fresh raspberry seed in soil</td>\n",
       "      <td>Taking the temperature for the day</td>\n",
       "      <td>Having a large breakfast</td>\n",
       "      <td>Putting a fresh raspberry seed in soil</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>9-976</td>\n",
       "      <td>Leaves are the part of a plant with the most a...</td>\n",
       "      <td>(A) root (B) bud (C) bacteria (D) specialized ...</td>\n",
       "      <td>Leaves are the part of a plant with the most a...</td>\n",
       "      <td>D</td>\n",
       "      <td>root</td>\n",
       "      <td>bud</td>\n",
       "      <td>bacteria</td>\n",
       "      <td>specialized compartment</td>\n",
       "      <td>specialized compartment</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>10-1041</td>\n",
       "      <td>Precipitation is the amount of</td>\n",
       "      <td>(A) dew point (B) raindrops (C) flooding (D) b...</td>\n",
       "      <td>Precipitation is the amount of (A) dew point (...</td>\n",
       "      <td>B</td>\n",
       "      <td>dew point</td>\n",
       "      <td>raindrops</td>\n",
       "      <td>flooding</td>\n",
       "      <td>barometric pressure</td>\n",
       "      <td>raindrops</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>10-525</td>\n",
       "      <td>Where could you expect to see a quail sleeping?</td>\n",
       "      <td>(A) box (B) field (C) ocean (D) tree</td>\n",
       "      <td>Where could you expect to see a quail sleeping...</td>\n",
       "      <td>B</td>\n",
       "      <td>box</td>\n",
       "      <td>field</td>\n",
       "      <td>ocean</td>\n",
       "      <td>tree</td>\n",
       "      <td>field</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                      Question Stem  \\\n",
       "3355  12-1201  Which are likely slightly larger in size than ...   \n",
       "2405   13-508            What does a stove generate for cooking?   \n",
       "4373   9-1064                      a plant needs this to survive   \n",
       "107     7-590                        An example of weathering is   \n",
       "3802   12-918  Echolocation is when animals detect objects by...   \n",
       "39     10-397  A large turbine in a field can produce electri...   \n",
       "2522    8-339                   Peat is an important factor when   \n",
       "3873    9-976  Leaves are the part of a plant with the most a...   \n",
       "1762  10-1041                     Precipitation is the amount of   \n",
       "2224   10-525    Where could you expect to see a quail sleeping?   \n",
       "\n",
       "                                                Choices  \\\n",
       "3355  (A) solid steel (B) solid wood (C) solid glaci...   \n",
       "2405  (A) ice (B) scorching temperatures (C) cold (D...   \n",
       "4373  (A) bugs to eat (B) liquid food (C) finger foo...   \n",
       "107   (A) sand (B) sharp boulders (C) tall trees (D)...   \n",
       "3802      (A) smell (B) vision (C) sight (D) screeching   \n",
       "39    (A) a breeze is floating (B) a child is crying...   \n",
       "2522  (A) Driving a new car (B) Putting a fresh rasp...   \n",
       "3873  (A) root (B) bud (C) bacteria (D) specialized ...   \n",
       "1762  (A) dew point (B) raindrops (C) flooding (D) b...   \n",
       "2224               (A) box (B) field (C) ocean (D) tree   \n",
       "\n",
       "                                      Complete Question Answer Key  \\\n",
       "3355  Which are likely slightly larger in size than ...          C   \n",
       "2405  What does a stove generate for cooking? (A) ic...          B   \n",
       "4373  a plant needs this to survive (A) bugs to eat ...          D   \n",
       "107   An example of weathering is (A) sand (B) sharp...          A   \n",
       "3802  Echolocation is when animals detect objects by...          D   \n",
       "39    A large turbine in a field can produce electri...          A   \n",
       "2522  Peat is an important factor when (A) Driving a...          B   \n",
       "3873  Leaves are the part of a plant with the most a...          D   \n",
       "1762  Precipitation is the amount of (A) dew point (...          B   \n",
       "2224  Where could you expect to see a quail sleeping...          B   \n",
       "\n",
       "                     OptionA                                   OptionB  \\\n",
       "3355            solid steel                                solid wood    \n",
       "2405                    ice                    scorching temperatures    \n",
       "4373            bugs to eat                               liquid food    \n",
       "107                    sand                            sharp boulders    \n",
       "3802                  smell                                    vision    \n",
       "39     a breeze is floating                         a child is crying    \n",
       "2522      Driving a new car    Putting a fresh raspberry seed in soil    \n",
       "3873                   root                                       bud    \n",
       "1762              dew point                                 raindrops    \n",
       "2224                    box                                     field    \n",
       "\n",
       "                                   OptionC                    OptionD  \\\n",
       "3355                       solid glaciers               solid mercury   \n",
       "2405                                 cold                    freezing   \n",
       "4373                          finger food        nutritional material   \n",
       "107                            tall trees                 coral reefs   \n",
       "3802                                sight                  screeching   \n",
       "39                       a dog is barking        clouds are very thin   \n",
       "2522   Taking the temperature for the day    Having a large breakfast   \n",
       "3873                             bacteria     specialized compartment   \n",
       "1762                             flooding         barometric pressure   \n",
       "2224                                ocean                        tree   \n",
       "\n",
       "                                        Result  LABEL  \n",
       "3355                           solid glaciers       2  \n",
       "2405                   scorching temperatures       1  \n",
       "4373                      nutritional material      3  \n",
       "107                                      sand       0  \n",
       "3802                                screeching      3  \n",
       "39                       a breeze is floating       0  \n",
       "2522   Putting a fresh raspberry seed in soil       1  \n",
       "3873                   specialized compartment      3  \n",
       "1762                                raindrops       1  \n",
       "2224                                    field       1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concated.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "with open('InferSent/encoder/data/openbook.txt') as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.strip())\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAExample(object):\n",
    "    \"\"\"A single training/test example for the SWAG dataset.\"\"\"\n",
    "    def __init__(self,\n",
    "                 QA_id,\n",
    "                 context_sentence,\n",
    "                 question_stem,\n",
    "                 ending_0,\n",
    "                 ending_1,\n",
    "                 ending_2,\n",
    "                 ending_3,\n",
    "                 label = None):\n",
    "        self.qa_id = QA_id\n",
    "        self.context_sentence = context_sentence\n",
    "        self.question_stem = question_stem\n",
    "        self.endings = [\n",
    "            ending_0,\n",
    "            ending_1,\n",
    "            ending_2,\n",
    "            ending_3,\n",
    "        ]\n",
    "        self.label = label\n",
    "\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 choices_features,\n",
    "                 label\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.choices_features = [\n",
    "            {\n",
    "                'input_ids': input_ids,\n",
    "                'input_mask': input_mask,\n",
    "                'segment_ids': segment_ids\n",
    "            }\n",
    "            for _, input_ids, input_mask, segment_ids in choices_features\n",
    "        ]\n",
    "        self.label = label        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def select_field(features, field):\n",
    "    return [\n",
    "        [\n",
    "            choice[field]\n",
    "            for choice in feature.choices_features\n",
    "        ]\n",
    "        for feature in features\n",
    "    ]\n",
    "\n",
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " examples = [\n",
    "        QAExample(\n",
    "            QA_id = concated['ID'].values,\n",
    "            context_sentence = sentences,\n",
    "            question_stem = concated[\"Question Stem\"].values,\n",
    "            ending_0 = concated['OptionA'].values,\n",
    "            ending_1 = concated['OptionB'].values,\n",
    "            ending_2 = concated['OptionC'].values,\n",
    "            ending_3 = concated['OptionD'].values,\n",
    "            label = concated[\"LABEL\"].values\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/27/2019 19:47:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ec2-user/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "features = []\n",
    "max_seq_length = 128\n",
    "for example_index, example in enumerate(examples):\n",
    "    context_sentence_tokens = tokenizer.tokenize(''.join(example.context_sentence))\n",
    "    question_stem_tokens = tokenizer.tokenize(''.join(example.question_stem))\n",
    "   \n",
    "    \n",
    "    \n",
    "    choices_features = [] \n",
    "    for ending_index, ending in enumerate(example.endings):\n",
    "        context_tokens_sentence = context_sentence_tokens[:]\n",
    "        ending_tokens = question_stem_tokens + tokenizer.tokenize(''.join(ending))        \n",
    "        _truncate_seq_pair(context_tokens_sentence, ending_tokens, max_seq_length - 3)\n",
    "        \n",
    "        \n",
    "        tokens = [\"[CLS]\"] + context_tokens_sentence + [\"[SEP]\"] + ending_tokens + [\"[SEP]\"]\n",
    "                                                          \n",
    "                                                                  \n",
    "        segment_ids = [0] * (len(context_tokens_sentence) + 2) + [1] * (len(ending_tokens) + 1)   \n",
    "     \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      \n",
    "        input_mask = [1] * len(input_ids)\n",
    "      \n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        choices_features.append((tokens, input_ids, input_mask, segment_ids))\n",
    "        \n",
    "                                                                  \n",
    "                                                                  \n",
    "        labels = example.label\n",
    "         \n",
    "                            \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                example_id = example.qa_id,\n",
    "                choices_features = choices_features,\n",
    "                label = labels\n",
    "            )\n",
    "        )  \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/27/2019 19:47:22 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: \n",
      "01/27/2019 19:47:22 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/ec2-user/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "01/27/2019 19:47:22 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/ec2-user/.pytorch_pretrained_bert/distributed_-1/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpvxww5oxk\n",
      "01/27/2019 19:47:26 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "01/27/2019 19:47:29 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultipleChoice not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "01/27/2019 19:47:29 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "local_rank = -1\n",
    "fp16 = \"\"\n",
    "no_cuda = \"\"\n",
    "gradient_accumulation_steps = 1\n",
    "seed = 42\n",
    "train_batch_size = 32\n",
    "do_train=True\n",
    "do_eval=True\n",
    "num_train_epochs = 30\n",
    "bert_model = 'bert-base-uncased'\n",
    "\n",
    "output_dir=os.getcwd() + \"models\"\n",
    "\n",
    "if local_rank == -1 or no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\",local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "    device, n_gpu, bool(local_rank != -1), fp16))\n",
    "\n",
    "if gradient_accumulation_steps < 1:\n",
    "    raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                        gradient_accumulation_steps))\n",
    "\n",
    "train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if not do_train and not do_eval:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_eval` must be True.\")\n",
    "\n",
    "if os.path.exists(output_dir) and os.listdir(output_dir):\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(output_dir))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "num_train_steps = int(\n",
    "        len(train) / train_batch_size / gradient_accumulation_steps * num_train_epochs)\n",
    "\n",
    "\n",
    "# Prepare model\n",
    "model = BertForMultipleChoice.from_pretrained('bert-base-uncased',\n",
    "                                              cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n",
    "                                              num_choices=4)\n",
    "\n",
    "# if fp16:\n",
    "#     model.half()\n",
    "# model.to(device)\n",
    "# if local_rank != -1:\n",
    "#     try:\n",
    "#         from apex.parallel import DistributedDataParallel as DDP\n",
    "#     except ImportError:\n",
    "#         raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "#     model = DDP(model)\n",
    "# elif n_gpu > 1:\n",
    "#     model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# local_rank = -1\n",
    "# fp16 = \"\"\n",
    "# no_cuda = \"\"\n",
    "# gradient_accumulation_steps = 1\n",
    "# seed = 42\n",
    "# train_batch_size = 32\n",
    "# do_train=True\n",
    "# do_eval=True\n",
    "# num_train_epochs = 30\n",
    "# bert_model = 'bert-base-uncased'\n",
    "\n",
    "# output_dir=os.getcwd() + \"models\"\n",
    "\n",
    "\n",
    "\n",
    "# model = BertForMultipleChoice.from_pretrained('bert-base-uncased',\n",
    "#                                               cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / 'distributed_{}'.format(local_rank),\n",
    "#                                               num_choices=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/27/2019 20:08:18 - INFO - __main__ -   ***** Running training *****\n",
      "01/27/2019 20:08:18 - INFO - __main__ -     Num examples = 4957\n",
      "01/27/2019 20:08:18 - INFO - __main__ -     Batch size = 32\n",
      "01/27/2019 20:08:18 - INFO - __main__ -     Num steps = 4647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 128]),\n",
       " torch.Size([4, 128]),\n",
       " torch.Size([4, 128]),\n",
       " torch.Size([4, 4957]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 5e-5\n",
    "warmup_proportion = 0.1\n",
    "loss_scale = 0\n",
    "train_features = features\n",
    "\n",
    "\n",
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "# hack to remove pooler, which is not used\n",
    "# thus it produce None grad that break apex\n",
    "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "\n",
    "if local_rank != -1:\n",
    "    t_total = t_total // torch.distributed.get_world_size()\n",
    "if fp16:\n",
    "    try:\n",
    "        from apex.optimizers import FP16_Optimizer\n",
    "        from apex.optimizers import FusedAdam\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                          lr=learning_rate,\n",
    "                          bias_correction=False,\n",
    "                          max_grad_norm=1.0)\n",
    "    if loss_scale == 0:\n",
    "        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "    else:\n",
    "        optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\n",
    "else:\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=learning_rate,\n",
    "                     warmup=warmup_proportion,\n",
    "                     t_total=t_total)  \n",
    "\n",
    "input_ids = select_field(train_features, 'input_ids')\n",
    "flat_list_input_ids = []\n",
    "for item in input_ids:\n",
    "    flat_list_input_ids.append(item)\n",
    "\n",
    "input_mask = select_field(train_features, 'input_mask')\n",
    "flat_list_input_mask = []\n",
    "for item in input_mask:\n",
    "    flat_list_input_mask.append(item)\n",
    "\n",
    "\n",
    "segment_ids = select_field(train_features, 'segment_ids')\n",
    "flat_list_segment_ids = []\n",
    "for item in segment_ids:\n",
    "    flat_list_segment_ids.append(item)\n",
    "\n",
    "# labels_y=[]\n",
    "# for item in train_features:\n",
    "#     for label in item.label:\n",
    "#         labels_y.append(label)\n",
    "\n",
    "global_step = 0\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train))\n",
    "logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "all_input_ids = torch.FloatTensor(input_ids[3])\n",
    "all_input_mask = torch.tensor(input_mask[3])\n",
    "all_segment_ids = torch.tensor(segment_ids[3])\n",
    "all_label = torch.tensor([f.label for f in train_features])\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n",
    "\n",
    "all_input_ids.size(),all_input_mask.size(),all_segment_ids.size(),all_label.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.optim import Adam\n",
    "\n",
    "# learning_rate = 5e-5\n",
    "# warmup_proportion = 0.1\n",
    "# loss_scale = 0\n",
    "# train_features = features\n",
    "\n",
    "# # Prepare optimizer\n",
    "# FULL_FINETUNING = True\n",
    "# if FULL_FINETUNING:\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#     optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "#     ]\n",
    "#     t_total = num_train_steps\n",
    "# else:\n",
    "#     param_optimizer = list(model.classifier.named_parameters()) \n",
    "#     optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "# optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "\n",
    "\n",
    "# input_ids = select_field(train_features, 'input_ids')\n",
    "# flat_list_input_ids = []\n",
    "# for item in input_ids:\n",
    "#     flat_list_input_ids.append(item)\n",
    "\n",
    "# input_mask = select_field(train_features, 'input_mask')\n",
    "# flat_list_input_mask = []\n",
    "# for item in input_mask:\n",
    "#     flat_list_input_mask.append(item)\n",
    "\n",
    "\n",
    "# segment_ids = select_field(train_features, 'segment_ids')\n",
    "# flat_list_segment_ids = []\n",
    "# for item in segment_ids:\n",
    "#     flat_list_segment_ids.append(item)\n",
    "\n",
    "# # labels_y=[]\n",
    "# # for item in train_features:\n",
    "# #     for label in item.label:\n",
    "# #         labels_y.append(label)\n",
    "\n",
    "# global_step = 0\n",
    "# logger.info(\"***** Running training *****\")\n",
    "# logger.info(\"  Num examples = %d\", len(train))\n",
    "# logger.info(\"  Batch size = %d\", train_batch_size)\n",
    "# logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "# all_input_ids = torch.tensor(flat_list_input_ids[3], dtype=torch.long)\n",
    "# all_input_mask = torch.tensor(flat_list_input_mask[3], dtype=torch.long)\n",
    "# all_segment_ids = torch.tensor(flat_list_segment_ids[3], dtype=torch.long)\n",
    "# all_label = torch.tensor([f.label for f in train_features], dtype=torch.long)\n",
    "# train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THC/generic/THCTensorCopy.cpp:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-a32b9ff8c670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnb_tr_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_tr_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-a32b9ff8c670>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnb_tr_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_tr_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THC/generic/THCTensorCopy.cpp:20"
     ]
    }
   ],
   "source": [
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\n",
    "  \n",
    "for _ in trange(3, desc=\"Epoch\"):\n",
    "    model.train() \n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        \n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "\n",
    "        if fp16 and loss_scale != 1.0:\n",
    "            # rescale loss for fp16 training\n",
    "            # see https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "            loss = loss * loss_scale\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        if fp16:\n",
    "            optimizer.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = learning_rate * warmup_linear(global_step/t_total, warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128]), torch.Size([10, 128]))"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_segment_ids.shape, all_input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = select_field(train_features, 'input_ids')\n",
    "len(input_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
